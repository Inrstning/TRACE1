verbose: True

# Job-level configuration options.
job:
  # Type of job to run. Possible values are "train", "eval", and "search". See
  # the corresponding configuration keys for mode information.
  type: train

  # Main device to use for this job (e.g., 'cpu', 'cuda', 'cuda:0')
  device: 'cuda'

  multi_gpu: True

# The seeds of the PRNGs can be set manually for (increased) reproducability.
# Use -1 to use default seed.
random_seed:
  python: -1
  torch: -1
  numpy: -1


## DATASET #####################################################################

dataset:
  # Specify a dataset here. There must be a folder of that name under "data/".
  # If this folder contains a dataset.yaml file, it overrides the defaults
  # specified below.
  name: 'toy'

  # Number of entities. If set to -1, automatically determined from the
  # entity_ids file (see below).
  num_entities: -1

  # Number of relations. If set to -1, automatically determined from the
  # relation_ids file (see below).
  num_relations: -1

  # A list of files associated with this dataset, each associated with a key.
  # Each entry must contain at least the filename and the type fields.
  files:
    # train, valid, and test are the keys used for the splits in training and
    # evaluation.
    #
    # The files are of type "triples" and contain tab-separated fields:
    # - 0: subject index
    # - 1: # relation index
    # - 2: object index
    # - 3-...: arbitrary metadata fields
    #
    # Indexes are assumed to be dense throughout.
    train:
      filename: train.del
      type: triples
    valid:
      filename: valid.del
      type: triples
    test:
      filename: test.del
      type: triples

    # Entity and relation ids files, which store the externally used ids for
    # each entity/relation. These files are optional for many models if
    # 'dataset.num_entitites' and 'dataset.num_relations' is specified.
    #
    # The map format uses two fields that are tab-separated:
    # - 0: internal entity/relation index (as in train/valid/test)
    # - 1: external id (interpreted as string)
    entity_ids:
      filename: entity_ids.del
      type: map
    relation_ids:
      filename: relation_ids.del
      type: map

    # Files that store human-readble string representations of each
    # entity/relation. These files are optional.
    #
    # Type can be map (field 0 is internal index) or idmap (field 0 is external
    # id).
    entity_strings:
      filename: entity_ids.del
      type: map
    relation_strings:
      filename: relation_ids.del
      type: map

    # Additional files can be added as needed
    +++: +++

  # Whether to store processed dataset files and indexes as binary files in the
  # dataset directory. This enables faster loading at the cost of storage space.
  # Note that the value specified here may be overwritten by dataset-specific choices
  # (in dataset.yaml).
  pickle: True

  # Additional dataset specific keys can be added as needed
  +++: +++

## MODEL #######################################################################


model: ''


## TRAINING ####################################################################

# Options of training jobs (job.type=="train")
train:
  # Split used for training (specified under 'dataset.files').
  split: train

  # Type of training job.
  # - KvsAll: scores each unique sp/po/so pair along with all possible completions.
  # - negative_sampling: scores each unique spo triple along with sampled corrupted
  #   triples
  # - 1vsAll: scores each spo triples against the complete set of s/p-corrputed triples
  #   (all treated negative)
  type: KvsAll


  loss: kl

  # Argument of loss function (if any). If .nan is specified, a default value is
  # used (as stated in parenthesis below).
  #
  # - margin_ranking (1.0): margin to use
  # - bce (0.0): offset to add to raw model scores before appyling the logistic
  #              function. This is useful esp. when the model only outputs negative
  loss_arg: .nan

  filt_loss: false

  # Maximum number of epochs used for training
  max_epochs: 20

  # Batch size used for training.
  batch_size: 100

  # Update frequency. n>1 to achieve gradient accumulation
  update_freq: 1

  # Number of workers used to construct batches. Leave at 0 for default (no
  # separate workers). On Windows, only 0 is supported.
  num_workers: 0

  # Optimizer used for training.
  optimizer: Adagrad           # sgd, adagrad, adam

  # Additional arguments for the optimizer. Arbitrary key-value pairs can be
  # added here and will be passed along to the optimizer. E.g., use entry lr:0.1
  # to set the learning rate to 0.1.
  optimizer_args:
    +++: +++

  # Learning rate scheduler to use. Any scheduler from torch.optim.lr_scheduler
  # can be used (e.g., ReduceLROnPlateau). When left empty, no LR scheduler is
  # used.
  lr_scheduler: ""

  # Additional arguments for the scheduler.
  lr_scheduler_args:
    +++: +++

  # When to write entries to the trace file.
  trace_level: epoch           # batch, epoch

  # When to create checkpoints
  checkpoint:
    # In addition the the checkpoint of the last epoch (which is transient),
    # create an additional checkpoint every this many epochs. Disable additional
    # checkpoints with 0.
    every: 5

    # Keep this many most recent additional checkpoints.
    keep: 3


  auto_correct: False

  # Abort training (with an error) when the value of the cost function becomes
  # not a number.
  abort_on_nan: True

  # If set, create a PDF of the compute graph (of first batch of first epoch).
  visualize_graph: False

  # Other options
  pin_memory: False


# Options for KvsAll training (train.type=="KvsAll")
KvsAll:

  label_smoothing: 0.0

  # Query types used during training. Here _ indicates the prediction target.
  # For example, sp_ means queries of form (s,p,?): predict all objects for each
  # distinct subject-predicate pair (s,p).
  query_types:
    sp_: True
    s_o: False
    _po: True

# Options for negative sampling training (train.type=="negative_sampling")
negative_sampling:
  # Negative sampler to use
  # - uniform  : samples entities/relations for corruption uniformly from the set
  #              of entities/relations
  # - frequency: samples entities/relations for corruption based on their relative
  #              frequency in the corresponding slot in the training data
  sampling_type: uniform

  # Options for sampling type frequency (negative_sampling.sampling_type=="frequency")
  frequency:
    # Smoothing constant to add to frequencies in training data (disable with
    # 0). Corresponds to a symmetric Dirichlet prior.
    smoothing: 1

  # Number of times each slot of each positive triple is corrupted by the
  # sampler to obtain negative triples.
  num_samples:
    s: 3
    p: 0          # -1 means: same as s
    o: -1         # -1 means: same as s

  # Whether to resample corrupted triples that occur in the training data (and
  # are hence positives). Can be set separately for each slot.
  filtering:
    s: False       # filter and resample for slot s
    p: False       # as above
    o: False       # as above

    split: ''      # split containing the positives; default is train.split

    # Implementation to use for filtering.
    # standard: use slow generic implementation, available for all samplers
    # fast: use specialized fast implementation, available for some samplers
    # fast_if_available: use fast implementation if available, else standard
    implementation: fast_if_available

  # Whether to share the s/p/o corruptions for all triples in the batch. This
  # can make training more efficient. Cannot be used with together with
  # filtering, but it is ensured that each triple does not get itself as a
  # negative.
  shared: False

  # Whether sampling should be performed with replacement. Without replacement
  # sampling is currently only supported for shared sampling.
  with_replacement: True

  # Implementation to use for the negative sampling job. Possible values are:
  # - triple: Scores every positive and negative triple in the batch
  # - all   : Scores against all possible targets and filters relevant scores
  #           out of the resulting score matrix
  # - batch : Scores against all targets contained in batch (or chunk) and
  #           filters relevant scores out of the resulting score matrix.
  # - auto  : Chooses best implementation based on num_samples.
  #           'batch' if shared sampling is activated or max(num_samples.s,
  #           num_samples.p, num_samples.o) > 30. 'triple' otherwise.
  #
  # 'batch' or 'auto' is recommended (faster) for models which have efficient
  # implementations to score many targets at once. For all other models, use
  # 'triple' (e.g., for TransE or RotatE in the current implementation).
  implementation: triple

  # Perform training in chunks of the specified size. When set, process each
  # batch in chunks of at most this size. This reduces memory consumption but
  # may increase runtime. Useful when there are many negative samples and/or
  # memory-intensive models are used. An alternative is to reduce the
  # batch_size.
  chunk_size: -1                  # default: no chunking


## VALIDATION AND EVALUATION ###################################################

# Options used for all evaluation jobs (job.type=="eval"). Also used during
# validation when training (unless overridden, see below). Right now, evaluation
# jobs compute a fixed set of metrics: MRR, HITS@k.
eval:
  # Split used for evaluation (specified under 'dataset.files').
  split: valid

  # Splits used to filter for filtered metrics. The split using for evaluation
  # (as set above) will be added automatically if not present.
  filter_splits: [ 'train', 'valid' ]

  # Whether test data should be used for filtering even if the current filter
  # splits do not contain it (most notably: during validation). When this is set
  # to True and "test" is not already a filter split, *additionally* produces
  # "filtered_with_test" metrics (such as MRR or HITS@k). Apparently, many
  # existing models have been trained with this set to True during model
  # selection and using a metric such as
  # mean_reciprocal_rank_filtered_with_test.
  filter_with_test: True

  # Time-aware filtering configuration options
  time_aware_filtering: False  # Enable time-aware filtering
  filter_with_same_timestamp: True  # Only filter triples with same timestamp

  # Type of evaluation (entity_ranking only at the moment)
  type: entity_ranking

  # How to handle cases with ties between the correct answer and other answers, e.g.,
  #  Query: (s, p, ?).
  #  Answers and score: a:10, b:10, c:10, d:11, e:9
  #  Correct: 'a'.
  tie_handling: rounded_mean_rank

  # Compute Hits@K for these choices of K
  hits_at_k_s: [1, 3, 10]

  # Batch size used during evaluation
  batch_size: 1024

  # Perform evaluation in chunks of the specified size. When set, score against
  # at most this many entities simultaneouly during prediction. This reduces
  # memory consumption but may increase runtime. Useful when there are many
  # entities and/or memory-intensive models are used.
  chunk_size: -1

  # Metrics are always computed over the entire evaluation data. Optionally,
  # certain more specific metrics can be computed in addition.
  metrics_per:
    head_and_tail: False          # head, tail; also applied to relation_type below
    relation_type: True          # 1-to-1, 1-to-N, N-to-1, N-to-N
    argument_frequency: False     # 25%, 50%, 75%, top quantiles per argument

  # Amount of tracing information being written. When set to "example", traces
  # the rank of the correct answer for each example.
  trace_level: example

  # Number of workers used to construct batches. Leave at 0 for default (no
  # separate workers). On Windows, only 0 is supported.
  num_workers: 0

  # Other options
  pin_memory: False

# Configuration options for model validation/selection during training. Applied
# in addition to the options set under "eval" above.
valid:
  # Split used for validation. If '', use eval.split, else override eval.split
  # during validation with this value.
  split: 'valid'

  # Validation is run every this many epochs during training (disable validation
  # with 0).
  every: 5

  # Name of the trace entry that holds the validation metric (higher value is
  # better)
  metric: mean_reciprocal_rank_filtered_with_test

  # If the above metric is not present in trace (e.g., because a custom metric
  # should be used), a Python expression to compute the metric. Can refer to
  # trace entries directly and to configuration options via config.
  # Example: 'math.sqrt(mean_reciprocal_rank) + config.get("user.par")'
  metric_expr: 'float("nan")'

  early_stopping:
    # Grace period of validation runs before a training run is stopped early
    # (disable early stopping with 0). If the value is set to n, then training is
    # stopped when there has been no improvement (compared to the best overall
    # result so far) in the validation metric during the last n validation runs.
    patience: 5

    # A minimum validation metric value threshold that should be reached after
    # n epochs, set to 0 epoch to turn off. Should be set very very conservatively
    # and the main purpose is for pruning completely useless hyperparameter
    # settings during hyper-parameter optimization.
    min_threshold:
      epochs: 0
      metric_value: 0.0

  # Amount of tracing information being written. When set to "example", traces
  # the rank of the correct answer for each example.
  trace_level: epoch


## EVALUATION ##################################################################


## HYPERPARAMETER SEARCH #######################################################

# Options of hyperparameter search jobs (job.type=="search").
search:
  # The type of search to run (see descriptions below). Possible values: manual,
  # grid, ax
  type: ax

  # Maximum number of parallel training jobs to run during a search.
  num_workers: 1

  # Device pool to use for training jobs. If this list is empty, `job.device` is
  # used for all parallel searches. Otherwise, the first `search.num_workers`